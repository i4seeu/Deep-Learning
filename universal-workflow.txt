##Universal workflow

1. Defining the problem and assembling a dataset
2. Choosing a measure of success
3. Deciding on an evaluation protocol
'''
Once you know what you’re aiming for, you must establish how you’ll measure your
current progress. We’ve previously reviewed three common evaluation protocols:
 Maintaining a hold-out validation set—The way to go when you have plenty of data
 Doing K-fold cross-validation—The right choice when you have too few samplesfor hold-out validation to be reliable
 Doing iterated K-fold validation—For performing highly accurate model evalua-tion when little data is available Just 
pick one of these. In most cases, the first will work well enough '''

4. Preparing your data

5. Developing a model that does better than a baseline
'''
Assuming that things go well, you need to make three key choices to build your
first working model:
 Last-layer activation—This establishes useful constraints on the network’s out-
put. For instance, the IMDB classification example used sigmoid in the last
layer; the regression example didn’t use any last-layer activation; and so on.
 Loss function—This should match the type of problem you’re trying to solve. For
instance, the IMDB example used binary_crossentropy , the regression example used mse , and so on.
 Optimization configuration—What optimizer will you use? What will its learning
rate be? In most cases, it’s safe to go with rmsprop and its default learning rate.
'''
Problem type | Last-layer activation | Loss function
Binary classification | sigmoid  | binary_crossentropy
Multiclass, single-label classification |softmax | categorical_crossentropy
Multiclass, multilabel classification | sigmoid | binary_crossentropy
Regression to arbitrary values | None | mse
Regression to values between 0 and 1 | sigmoid | mse or binary_crossentropy

6. Scaling up: developing a model that overfits.

To figure out how big a model you’ll need, you must develop a model that overfits.
This is fairly easy:

1. Add layers.
2. Make the layers bigger.
3. Train for more epochs.

Always monitor the training loss and validation loss, as well as the training and valida-
tion values for any metrics you care about. When you see that the model’s perfor-
mance on the validation data begins to degrade, you’ve achieved overfitting.

7. Regularizing your model and tuning your hyperparameters

This step will take the most time: you’ll repeatedly modify your model, train it, evalu-
ate on your validation data (not the test data, at this point), modify it again, and
repeat, until the model is as good as it can get. These are some things you should try:
 1. Add dropout.
 2. Try different architectures: add or remove layers.
 3. Add L1 and/or L2 regularization.

Try different hyperparameters (such as the number of units per layer or the
learning rate of the optimizer) to find the optimal configuration.
 Optionally, iterate on feature engineering: add new features, or remove fea-
tures that don’t seem to be informative